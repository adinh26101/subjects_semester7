{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This lab is adapted from programming assignments of the class \"Neural Networks and Deep Learning\" on Coursera\n",
    "## https://www.coursera.org/learn/neural-networks-deep-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-08T16:40:03.531861Z",
     "start_time": "2022-09-08T16:40:03.519770Z"
    }
   },
   "source": [
    "After finishing this lab, we would be familiar with:\n",
    "- Some basic functions and operators widely used in machine learning/ deep learning and how to implement them using Numpy\n",
    "- Vector/ matrix operations and broadcasting \n",
    "- Writing efficient code by using vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You only need to modify \"(≈ X lines of code)\" in the below exercises. Bear in mind that\n",
    "X is our approximation for the work needed to be done, \n",
    "you definetely can write shorter/longer code.\n",
    "\n",
    "\n",
    "Start by printing \"Lab 1 is awesome\" in the next cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-09T00:19:48.437382Z",
     "start_time": "2022-09-09T00:19:48.434922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# (≈ 1 line of code)\n",
    "# test = \n",
    "# YOUR CODE STARTS HERE\n",
    "\n",
    "# YOUR CODE ENDS HERE\n",
    "\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-08T16:42:12.937977Z",
     "start_time": "2022-09-08T16:42:12.932946Z"
    }
   },
   "source": [
    "**Expected output**:\n",
    "Lab 1 is awesome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-09T00:19:48.515741Z",
     "start_time": "2022-09-09T00:19:48.438925Z"
    }
   },
   "outputs": [],
   "source": [
    "# You need these libraries for following exercises\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "from test_lab_1 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-08T12:24:48.832321Z",
     "start_time": "2022-09-08T12:24:48.825356Z"
    }
   },
   "source": [
    "## Building basic functions with Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Basic sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$sigmoid(x) = \\frac{1}{1+e^{-x}}$ also known as logistic function. Sigmoid function is broadly used not only in machine learning (logistic regression) but also in deep learning as activation function.\n",
    "\n",
    "You need to build a function that returns the sigmoid value of a real number x. Use math.exp(x) for exponential function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-09T00:19:48.520798Z",
     "start_time": "2022-09-09T00:19:48.518169Z"
    }
   },
   "outputs": [],
   "source": [
    "def basic_sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute sigmoid of x.\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    assert type(x) == int or type(x) == float, 'Input must be integer or real number'\n",
    "    \n",
    "    # (≈ 1 line of code)\n",
    "    # s = \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-09T00:19:48.525409Z",
     "start_time": "2022-09-09T00:19:48.523046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic_sigmoid(1) = 0.7310585786300049\n",
      "All test cases are pass.\n"
     ]
    }
   ],
   "source": [
    "print(\"basic_sigmoid(1) = \" + str(basic_sigmoid(1)))\n",
    "\n",
    "test_basic_sigmoid(basic_sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T19:14:48.435539Z",
     "start_time": "2022-09-06T19:14:48.429938Z"
    }
   },
   "source": [
    "\"math\" is a great library for mathematical tasks but it can only deal with scalar. In practice, machine learning/ deep learning inputs are vectors and matrices. That's why Numpy is more advantageous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-08T17:14:32.184697Z",
     "start_time": "2022-09-08T17:14:32.175353Z"
    }
   },
   "source": [
    "Implement the sigmoid function using Numpy. The input x can be either a scalar, a vector or a matrix\n",
    "\n",
    "$$\n",
    "\\text{For } x \\in \\mathbb{R}^{m \\times n} \\text{,     } sigmoid(x) = sigmoid\n",
    "\\left(\\begin{array}{cccc}\n",
    "a_{11} & a_{12} & \\cdots & a_{1n} \\\\\n",
    "a_{21} & a_{22} & \\cdots & a_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a_{m1} & a_{m2} & \\cdots & a_{mn}\n",
    "\\end{array}\\right)\n",
    "=\n",
    "\\left(\\begin{array}{cccc}\n",
    "\\frac{1}{1+e^{-a_{11}}} & \\frac{1}{1+e^{-a_{12}}} & \\cdots & \\frac{1}{1+e^{-a_{1n}}} \\\\\n",
    "\\frac{1}{1+e^{-a_{21}}} & \\frac{1}{1+e^{-a_{22}}} & \\cdots & \\frac{1}{1+e^{-a_{2n}}} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\frac{1}{1+e^{-a_{m1}}} & \\frac{1}{1+e^{-a_{m2}}} & \\cdots & \\frac{1}{1+e^{-a_{mn}}}\n",
    "\\end{array}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-09T00:19:48.529443Z",
     "start_time": "2022-09-09T00:19:48.527034Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    assert type(x) == int or type(x) == float or type(x) == np.ndarray, \\\n",
    "            'Input must be integer/ real number/ numpy array'\n",
    "    \n",
    "    # (≈ 1 line of code)\n",
    "    # s = \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-09T00:19:48.533565Z",
     "start_time": "2022-09-09T00:19:48.530745Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(t_x) = [0.73105858 0.88079708 0.95257413]\n",
      "All test cases are pass.\n"
     ]
    }
   ],
   "source": [
    "t_x = np.array([1, 2, 3])\n",
    "print(\"sigmoid(t_x) = \" + str(sigmoid(t_x)))\n",
    "\n",
    "test_sigmoid(sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may know, gradient plays a key role in backpropagation algorithm. In this exercise, you need to implement the sigmoid_derivative() to compute the derivative of sigmoid function with respect to its input.\n",
    "\n",
    "$$sigmoid\\_derivative(x) = \\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\\tag{2}$$\n",
    "\n",
    "Your code may involve two steps:\n",
    "1. Compute sigmoid of x. You can use the function in previous exercise\n",
    "2. Compute derivative of sigmoid(x): $\\sigma'(x) = s(1-s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-09T00:19:48.538417Z",
     "start_time": "2022-09-09T00:19:48.535373Z"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    Compute the gradient (also called the slope or derivative) of the sigmoid function with respect to its input x.\n",
    "    You can store the output of the sigmoid function into variables and then use it to calculate the gradient.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array\n",
    "\n",
    "    Return:\n",
    "    ds -- Your computed gradient.\n",
    "    \"\"\"\n",
    "    assert type(x) == int or type(x) == float or type(x) == np.ndarray, \\\n",
    "            'Input must be integer/ real number/ numpy array'\n",
    "    \n",
    "    #(≈ 2 lines of code)\n",
    "    # s = \n",
    "    # ds = \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-09T00:19:48.544415Z",
     "start_time": "2022-09-09T00:19:48.541682Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid_derivative(t_x) = [0.19661193 0.10499359 0.04517666]\n",
      "All test cases are pass.\n"
     ]
    }
   ],
   "source": [
    "t_x = np.array([1, 2, 3])\n",
    "print (\"sigmoid_derivative(t_x) = \" + str(sigmoid_derivative(t_x)))\n",
    "\n",
    "test_sigmoid_derivative(sigmoid_derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many machine learning algorithms, especially in computer vision tasks, before feeding your data to the model,\n",
    "you need to reshape or change the input dimension. For example, an RGB image can be represented by a \n",
    "3-dimensional (3D) array of shape (length, height, depth = 3) might need to be converted to a vector of shape \n",
    "(length * height * 3, 1) so that the algorithm can \"read\" it.\n",
    "\n",
    "Implenment image2vector() function that convert a 3D (length, height, 3) array to 1D (length * height * 3, 1) one. You may find the built-in function \"shape\" useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-09T00:19:48.549028Z",
     "start_time": "2022-09-09T00:19:48.546689Z"
    }
   },
   "outputs": [],
   "source": [
    "def image2vector(image):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    image -- a numpy array of shape (length, height, depth)\n",
    "    \n",
    "    Returns:\n",
    "    v -- a vector of shape (length*height*depth, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    assert len(t_image.shape) == 3, 'Image must be a numpy array of shape (length, height, depth)'\n",
    "    \n",
    "    # (≈ 1 line of code)\n",
    "    # v =\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-09T00:19:48.554232Z",
     "start_time": "2022-09-09T00:19:48.550170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image2vector(image) = [[0.67826139]\n",
      " [0.29380381]\n",
      " [0.90714982]\n",
      " [0.52835647]\n",
      " [0.4215251 ]\n",
      " [0.45017551]\n",
      " [0.92814219]\n",
      " [0.96677647]\n",
      " [0.85304703]\n",
      " [0.52351845]\n",
      " [0.19981397]\n",
      " [0.27417313]\n",
      " [0.60659855]\n",
      " [0.00533165]\n",
      " [0.10820313]\n",
      " [0.49978937]\n",
      " [0.34144279]\n",
      " [0.94630077]]\n",
      "All test cases are pass.\n"
     ]
    }
   ],
   "source": [
    "# This is a 3 by 3 by 2 array, typically images will be (num_px_x, num_px_y,3) where 3 represents the RGB values\n",
    "t_image = np.array([[[ 0.67826139,  0.29380381],\n",
    "                     [ 0.90714982,  0.52835647],\n",
    "                     [ 0.4215251 ,  0.45017551]],\n",
    "\n",
    "                   [[ 0.92814219,  0.96677647],\n",
    "                    [ 0.85304703,  0.52351845],\n",
    "                    [ 0.19981397,  0.27417313]],\n",
    "\n",
    "                   [[ 0.60659855,  0.00533165],\n",
    "                    [ 0.10820313,  0.49978937],\n",
    "                    [ 0.34144279,  0.94630077]]])\n",
    "\n",
    "print (\"image2vector(image) = \" + str(image2vector(t_image)))\n",
    "\n",
    "test_image2vector(image2vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data normalizing is a well-known technique we usually use in Machine Learning models. Normalization helps \n",
    "gradient descent converges faster (refer to this paper if you want to learn more\n",
    "https://arxiv.org/abs/1502.03167). Here, by normalization we mean changing x to $ \\frac{x}{\\| x\\|} $ (dividing each row vector of x by its norm).\n",
    "\n",
    "For example, if \n",
    "$$x = \\begin{bmatrix}\n",
    "        0 & 6 & 8 \\\\\n",
    "        3 & 1 & 2 \\\\\n",
    "\\end{bmatrix}$$ \n",
    "then \n",
    "$$\\| x\\| = \\text{np.linalg.norm(x, axis=1, keepdims=True)} = \\begin{bmatrix}\n",
    "    10 \\\\\n",
    "    \\sqrt{14} \\\\\n",
    "\\end{bmatrix}$$\n",
    "and\n",
    "$$ x\\_normalized = \\frac{x}{\\| x\\|} = \\begin{bmatrix}\n",
    "    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\n",
    "    \\frac{3}{\\sqrt{14}} & \\frac{1}{\\sqrt{14}} & \\frac{2}{\\sqrt{14}} \\\\\n",
    "\\end{bmatrix}$$ \n",
    "\n",
    "Note that you can divide matrices of different sizes and it works fine: this is called broadcasting, `keepdims=True` the result will broadcast correctly against the original x.\n",
    "\n",
    "Implement normalize_rows() to normalize the rows of a matrix. Note that after normalizing, each row of x should be \n",
    "a vector of unit length (meaning length = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-09T00:19:48.557929Z",
     "start_time": "2022-09-09T00:19:48.555562Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_rows(x):\n",
    "    \"\"\"\n",
    "    Implement a function that normalizes each row of the matrix x (to have unit length).\n",
    "    \n",
    "    Argument:\n",
    "    x -- A numpy matrix of shape (n, m)\n",
    "    \n",
    "    Returns:\n",
    "    x -- The normalized (by row) numpy matrix. You are allowed to modify x.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert len(x.shape) == 2, 'Input must be a numpy array of shape (n,m)'\n",
    "    \n",
    "    #(≈ 2 lines of code)\n",
    "    # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n",
    "    # x_norm =\n",
    "    # Divide x by its norm.\n",
    "    # x =\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-09T00:19:48.562183Z",
     "start_time": "2022-09-09T00:19:48.559334Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalizeRows(x) = [[0.         0.6        0.8       ]\n",
      " [0.13736056 0.82416338 0.54944226]]\n",
      "All test cases are pass.\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0, 3, 4],\n",
    "              [1, 6, 4]])\n",
    "print(\"normalizeRows(x) = \" + str(normalize_rows(x)))\n",
    "\n",
    "test_normalize_rows(normalize_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-08T19:59:51.706844Z",
     "start_time": "2022-09-08T19:59:51.684451Z"
    }
   },
   "source": [
    "Softmax is a normalizing function that is really helpful in many multiclass classification problems.\n",
    "Implement softmax function with following instructions:\n",
    "\n",
    "- $\\text{for } x \\in \\mathbb{R}^{1\\times n} \\text{,     }$\n",
    "\n",
    "\\begin{align*}\n",
    " softmax(x) &= softmax\\left(\\begin{bmatrix}\n",
    "    x_1  &&\n",
    "    x_2 &&\n",
    "    ...  &&\n",
    "    x_n  \n",
    "\\end{bmatrix}\\right) \\\\&= \\begin{bmatrix}\n",
    "    \\frac{e^{x_1}}{\\sum_{j}e^{x_j}}  &&\n",
    "    \\frac{e^{x_2}}{\\sum_{j}e^{x_j}}  &&\n",
    "    ...  &&\n",
    "    \\frac{e^{x_n}}{\\sum_{j}e^{x_j}} \n",
    "\\end{bmatrix} \n",
    "\\end{align*}\n",
    "\n",
    "- $\\text{for a matrix } x \\in \\mathbb{R}^{m \\times n} \\text{,  $x_{ij}$ maps to the element in the $i^{th}$ row and $j^{th}$ column of $x$, thus we have: }$  \n",
    "\n",
    "\\begin{align*}\n",
    "softmax(x) &= softmax\\begin{bmatrix}\n",
    "            x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\n",
    "            x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\n",
    "            \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "            x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n",
    "            \\end{bmatrix} \\\\ \\\\&= \n",
    " \\begin{bmatrix}\n",
    "    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\n",
    "    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n",
    "\\end{bmatrix} \\\\ \\\\ &= \\begin{pmatrix}\n",
    "    softmax\\text{(first row of x)}  \\\\\n",
    "    softmax\\text{(second row of x)} \\\\\n",
    "    \\vdots  \\\\\n",
    "    softmax\\text{(last row of x)} \\\\\n",
    "\\end{pmatrix} \n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-09T00:19:48.565999Z",
     "start_time": "2022-09-09T00:19:48.563617Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Calculates the softmax for each row of the input x.\n",
    "\n",
    "    Your code should work for a row vector and also for matrices of shape (m,n).\n",
    "\n",
    "    Argument:\n",
    "    x -- A numpy matrix of shape (m,n)\n",
    "\n",
    "    Returns:\n",
    "    s -- A numpy matrix equal to the softmax of x, of shape (m,n)\n",
    "    \"\"\"\n",
    "    \n",
    "    assert len(x.shape) == 2, 'Input must be numpy array of shape (m,n)'\n",
    "    \n",
    "    #(≈ 3 lines of code)\n",
    "    # Apply exp() element-wise to x. Use np.exp(...).\n",
    "    # x_exp = ...\n",
    "\n",
    "    # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).\n",
    "    # x_sum = ...\n",
    "    \n",
    "    # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.\n",
    "    # s = ...\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-09T00:19:48.570590Z",
     "start_time": "2022-09-09T00:19:48.567483Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax(x) = [[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04\n",
      "  1.21052389e-04]\n",
      " [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04\n",
      "  8.01252314e-04]]\n",
      "All test cases are pass.\n"
     ]
    }
   ],
   "source": [
    "t_x = np.array([[9, 2, 5, 0, 0],\n",
    "                [7, 5, 0, 0 ,0]])\n",
    "print(\"softmax(x) = \" + str(softmax(t_x)))\n",
    "\n",
    "test_softmax(softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization is a technique which helps us to speed up our code by removing loops. You can understand by looking\n",
    "at the following examples of computational time of dot, outer and elementwise product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-09T00:19:48.583569Z",
     "start_time": "2022-09-09T00:19:48.572534Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation Time: \n",
      "\n",
      " + Dot Product:\n",
      "   - Classic implementation: 0.42999999999993044ms\n",
      "   - Vectorized implementation: 0.3210000000000157ms\n",
      "\n",
      " + Outer Product:\n",
      "   - Classic implementation: 0.5569999999999187ms\n",
      "   - Vectorized implementation: 0.37399999999998546ms\n",
      "\n",
      " + Elementwise multiplication:\n",
      "   - Classic implementation: 0.2929999999999877ms\n",
      "   - Vectorized implementation: 0.22199999999994446ms\n",
      "\n",
      " + General Dot Product:\n",
      "   - Classic implementation: 0.5349999999999522ms\n",
      "   - Vectorized implementation: 1.2159999999999949ms\n"
     ]
    }
   ],
   "source": [
    "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
    "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
    "\n",
    "print(\"Computation Time: \")\n",
    "\n",
    "print(\"\\n + Dot Product:\")\n",
    "### CLASSIC DOT PRODUCT OF VECTORS IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "dot = 0\n",
    "\n",
    "for i in range(len(x1)):\n",
    "    dot += x1[i] * x2[i]\n",
    "toc = time.process_time()\n",
    "print (\"   - Classic implementation: \" + str(1000 * (toc - tic)) + \"ms\")\n",
    "\n",
    "### VECTORIZED DOT PRODUCT OF VECTORS ###\n",
    "tic = time.process_time()\n",
    "dot = np.dot(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"   - Vectorized implementation: \" + str(1000 * (toc - tic)) + \"ms\")\n",
    "\n",
    "print(\"\\n + Outer Product:\")\n",
    "### CLASSIC OUTER PRODUCT IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "outer = np.zeros((len(x1), len(x2))) # we create a len(x1)*len(x2) matrix with only zeros\n",
    "\n",
    "for i in range(len(x1)):\n",
    "    for j in range(len(x2)):\n",
    "        outer[i,j] = x1[i] * x2[j]\n",
    "toc = time.process_time()\n",
    "print (\"   - Classic implementation: \" + str(1000 * (toc - tic)) + \"ms\")\n",
    "\n",
    "### VECTORIZED OUTER PRODUCT ###\n",
    "tic = time.process_time()\n",
    "outer = np.outer(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"   - Vectorized implementation: \" + str(1000 * (toc - tic)) + \"ms\")\n",
    "\n",
    "print(\"\\n + Elementwise multiplication:\")\n",
    "### CLASSIC ELEMENTWISE IMPLEMENTATION ###\n",
    "tic = time.process_time()\n",
    "mul = np.zeros(len(x1))\n",
    "\n",
    "for i in range(len(x1)):\n",
    "    mul[i] = x1[i] * x2[i]\n",
    "toc = time.process_time()\n",
    "print (\"   - Classic implementation: \" + str(1000 * (toc - tic)) + \"ms\")\n",
    "\n",
    "### VECTORIZED ELEMENTWISE MULTIPLICATION ###\n",
    "tic = time.process_time()\n",
    "mul = np.multiply(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"   - Vectorized implementation: \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "print(\"\\n + General Dot Product:\")\n",
    "### CLASSIC GENERAL DOT PRODUCT IMPLEMENTATION ###\n",
    "W = np.random.rand(3,len(x1)) # Random 3*len(x1) numpy array\n",
    "tic = time.process_time()\n",
    "gdot = np.zeros(W.shape[0])\n",
    "\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(len(x1)):\n",
    "        gdot[i] += W[i,j] * x1[j]\n",
    "toc = time.process_time()\n",
    "print (\"   - Classic implementation: \" + str(1000 * (toc - tic)) + \"ms\")\n",
    "\n",
    "### VECTORIZED GENERAL DOT PRODUCT ###\n",
    "tic = time.process_time()\n",
    "dot = np.dot(W,x1)\n",
    "toc = time.process_time()\n",
    "print (\"   - Vectorized implementation: \" + str(1000 * (toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, vectorized implementation yields faster and cleaner code. For a bigger data, the difference in running time become enormous. Vectorization is a technique that you don't want to forget when dealing with big data, since a non-computationally-optimal calculation can lead to a huge bottleneck in your algorithm and can result in a model that take ages to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing L1 and L2 loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-08T21:14:55.103697Z",
     "start_time": "2022-09-08T21:14:55.096166Z"
    }
   },
   "source": [
    "Every machine learning algorithm need (a) loss function. In supervised learning, loss function is defined as the difference of your predicted output ($ \\hat{y} $) and the true value ($y$). \n",
    "Implement the L1 loss function:\n",
    "$$\\begin{align*} & L_1(\\hat{y}, y) = \\sum_{i=0}^{m-1}|y^{(i)} - \\hat{y}^{(i)}| \\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-09T00:19:48.587620Z",
     "start_time": "2022-09-09T00:19:48.585048Z"
    }
   },
   "outputs": [],
   "source": [
    "def L1(yhat, y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    yhat -- vector of size m (predicted labels)\n",
    "    y -- vector of size m (true labels)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- the value of the L1 loss function defined above\n",
    "    \"\"\"\n",
    "    \n",
    "    assert type(yhat) == type(y) == np.ndarray, 'Input must be numpy array'\n",
    "    assert len(yhat) == len(y), 'yhat and y must have same size'\n",
    "    \n",
    "    #(≈ 1 line of code)\n",
    "    # loss = \n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-09T00:19:48.591936Z",
     "start_time": "2022-09-09T00:19:48.589370Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 = 1.1\n",
      "All test cases are pass.\n"
     ]
    }
   ],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L1 = \" + str(L1(yhat, y)))\n",
    "\n",
    "test_L1(L1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='#1'></a>\n",
    "### L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-08T21:26:23.997913Z",
     "start_time": "2022-09-08T21:26:23.990422Z"
    }
   },
   "source": [
    "Implement the numpy vectorized version of the L2 loss. You may find the function np.dot() useful. As a reminder, if $x = [x_1, x_2, ..., x_n]$, then `np.dot(x,x)` = $\\sum_{j=0}^n x_j^{2}$. \n",
    "\n",
    "- L2 loss is defined as $$\\begin{align*} & L_2(\\hat{y},y) = \\sum_{i=0}^{m-1}(y^{(i)} - \\hat{y}^{(i)})^2 \\end{align*}\\tag{7}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-09T00:19:48.595863Z",
     "start_time": "2022-09-09T00:19:48.593408Z"
    }
   },
   "outputs": [],
   "source": [
    "def L2(yhat, y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    yhat -- vector of size m (predicted labels)\n",
    "    y -- vector of size m (true labels)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- the value of the L2 loss function defined above\n",
    "    \"\"\"\n",
    "    assert type(yhat) == type(y) == np.ndarray, 'Input must be numpy array'\n",
    "    assert len(yhat) == len(y), 'yhat and y must have same size'\n",
    "    \n",
    "    #(≈ 1 line of code)\n",
    "    # loss = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-09T00:19:48.599665Z",
     "start_time": "2022-09-09T00:19:48.596975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 = 0.43\n",
      "All test cases are pass.\n"
     ]
    }
   ],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "\n",
    "print(\"L2 = \" + str(L2(yhat, y)))\n",
    "\n",
    "test_L2(L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Takeaway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This lab introduces you small but essential \"pieces\" (numpy functions, data processing and normalizing, L1/L2 loss functions,...) which help you to build a full-fledged machine learning/ deep learning model\n",
    "- Broadcasting and vectorization are techiniques that you need to get familiar with\n",
    "- Avoid hardcoding (In image2vector exercise, you can't pass all the test cases if you try to hardcode the image's dimensions, using \"shape\" to look up the quantities you need is the best solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
